{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "14e3639c-c82f-494e-8532-763e35caf1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "from utils import load_datasets\n",
    "from utils import make_loader, DataLoader\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57079dc6-f53d-4a7e-ad5b-a82cb5256569",
   "metadata": {},
   "source": [
    "# Count pixels per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3dc3f974-b23f-4dc1-8663-e6042f5384ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_path = '/palma/scratch/tmp/j_sten07/data/FloodNet/512px'\n",
    "\n",
    "patch_size = 512\n",
    "NUM_CLASSES = 10\n",
    "classes = 'floodnet'\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = load_datasets(data_path, random_split = True, classes = classes, patch_size=patch_size, normalize=False)#, only_test = True)\n",
    "train_loader, val_loader, test_loader = make_loader(train_dataset, validation_dataset, test_dataset)\n",
    "\n",
    "# test_dataset = load_datasets(data_path, random_split = True, classes = 'floodnet', patch_size=patch_size, normalize=False, only_test = True)\n",
    "# test_loader = DataLoader(test_dataset)\n",
    "# # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c72628f1-e009-4ab6-9c62-13c3239f7ca5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([     0, 123506,      0,      0,      0,      0,  64845,    197,      0,\n",
       "         73596])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.flatten(train_dataset[1][1]).bincount(minlength=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25507f0a-4e0b-4217-9aa8-e9c3dbbe1ff4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.5619e+08, 3.0361e+08, 4.6908e+08, 5.3199e+08, 7.6801e+08, 1.6679e+09,\n",
      "         2.5296e+09, 2.7787e+07, 3.0649e+07, 7.9904e+09],\n",
      "        [9.0506e+07, 1.0550e+08, 1.5345e+08, 1.8420e+08, 2.6361e+08, 5.5256e+08,\n",
      "         8.7632e+08, 9.4109e+06, 1.0617e+07, 2.6124e+09],\n",
      "        [1.4076e+08, 1.4228e+08, 1.6967e+08, 2.0183e+08, 3.4384e+08, 7.0855e+08,\n",
      "         1.1369e+09, 1.1586e+07, 1.2741e+07, 3.1979e+09]])\n"
     ]
    }
   ],
   "source": [
    "class_count = torch.zeros(3, NUM_CLASSES)\n",
    "for i in range(len(train_dataset)):\n",
    "    class_count[0] += torch.flatten(train_dataset[i][1]).bincount(minlength=NUM_CLASSES)\n",
    "for i in range(len(validation_dataset)):\n",
    "    class_count[1] += torch.flatten(validation_dataset[i][1]).bincount(minlength=NUM_CLASSES)\n",
    "for i in range(len(test_dataset)):\n",
    "    class_count[2] += torch.flatten(test_dataset[i][1]).bincount(minlength=NUM_CLASSES)\n",
    "    \n",
    "print(class_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f461a3ca-18c4-479d-8dfe-394291d353fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(class_count.numpy()).to_csv('../results/class_count'+classes+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f106463-8c84-4414-a1bb-8b45c215e4d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0176, 0.0208, 0.0322, 0.0365, 0.0527, 0.1144, 0.1736, 0.0019, 0.0021,\n",
       "         0.5482],\n",
       "        [0.0186, 0.0217, 0.0316, 0.0379, 0.0543, 0.1137, 0.1804, 0.0019, 0.0022,\n",
       "         0.5377],\n",
       "        [0.0232, 0.0235, 0.0280, 0.0333, 0.0567, 0.1168, 0.1874, 0.0019, 0.0021,\n",
       "         0.5272]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_count/class_count.sum(1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af6c90-9843-4dbb-8f50-e57d65619dd6",
   "metadata": {},
   "source": [
    "# Calculate mean and standard deviation of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2df5dabc-8992-4a2f-8de0-a170da1cd753",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = 0.\n",
    "std = 0.\n",
    "for images, _ in train_loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1) # reshape: make W x H one dimension\n",
    "    mean += images.mean(2).sum(0) # get mean per channel (dim 2) and sum them up for all batch elements (per channel -> dim 0)\n",
    "    std += images.std(2).sum(0)\n",
    "for images, _ in val_loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "for images, _ in test_loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    mean += images.mean(2).sum(0)\n",
    "    std += images.std(2).sum(0)\n",
    "    \n",
    "\n",
    "mean_all = mean/(len(train_loader.dataset)+len(val_loader.dataset)+len(test_loader.dataset))\n",
    "std_all = std/(len(train_loader.dataset)+len(val_loader.dataset)+len(test_loader.dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b920dd8f-23fa-42a4-b897-5bc53221bdc5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3823, 0.3625, 0.3364])\n",
      "tensor([0.1172, 0.1167, 0.1203])\n"
     ]
    }
   ],
   "source": [
    "print(mean_all)\n",
    "print(std_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fda8bcb-ec9f-4fee-9f56-28d11437b78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3791, 0.3599, 0.3333])\n",
      "tensor([0.1176, 0.1166, 0.1203])\n"
     ]
    }
   ],
   "source": [
    "train_mean = 0.\n",
    "train_std = 0.\n",
    "\n",
    "for images, _ in train_loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1) # reshape: make W x H one dimension\n",
    "    train_mean += images.mean(2).sum(0) # get mean per channel (dim 2) and sum them up for all batch elements (per channel -> dim 0)\n",
    "    train_std += images.std(2).sum(0)\n",
    "for images, _ in val_loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    train_mean += images.mean(2).sum(0)\n",
    "    train_std += images.std(2).sum(0)\n",
    "\n",
    "    \n",
    "\n",
    "train_mean = train_mean/(len(train_loader.dataset)+len(val_loader.dataset))\n",
    "train_std = train_std/(len(train_loader.dataset)+len(val_loader.dataset))\n",
    "print(train_mean)\n",
    "print(train_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e596255a-dd5d-46d7-b451-e25ebbcd8090",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4731, 0.3206, 0.3182])\n",
      "tensor([0.1970, 0.1306, 0.1276])\n"
     ]
    }
   ],
   "source": [
    "test_mean = 0.\n",
    "test_std = 0.    \n",
    "for images, _ in test_loader:\n",
    "    batch_samples = images.size(0) # batch size (the last batch can have smaller size!)\n",
    "    images = images.view(batch_samples, images.size(1), -1)\n",
    "    test_mean += images.mean(2).sum(0)\n",
    "    test_std += images.std(2).sum(0)\n",
    "    \n",
    "test_mean = test_mean/(len(test_loader.dataset))\n",
    "test_std = test_std/(len(test_loader.dataset))\n",
    "print(test_mean)\n",
    "print(test_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "134e9a33-7172-4810-a567-4c0ce8597e84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "norms = {\n",
    "    'imagenet': {'mean':(0.485, 0.456, 0.406), 'std':(0.229, 0.224, 0.225)},\n",
    "    'potsdam': {'mean':(0.349, 0.371, 0.347), 'std':(0.1196, 0.1164, 0.1197)},\n",
    "    'potsdam_irrg': {'mean':(0.3823, 0.3625, 0.3364), 'std':(0.1172, 0.1167, 0.1203)},\n",
    "    'floodnet': {'mean':(0.4159, 0.4499, 0.3466), 'std':(0.1297, 0.1197, 0.1304)},\n",
    "    'vaihingen': {'mean':(0.4731, 0.3206, 0.3182), 'std':(0.1970, 0.1306, 0.1276)},\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
